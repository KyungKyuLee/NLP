{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPTPnWUXM7MEt+5mWAB8wiD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyungKyuLee/NLP/blob/master/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6jMBEH2sgCX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Deep Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSwm_1ok59K7",
        "colab_type": "text"
      },
      "source": [
        "## Deep learning\n",
        "Deep learning is part of a broader family of machine learning methods based on artifical neural networks with representing learning.\n",
        "\n",
        "Deep learning may be the most famous kind of machine learning. The base structure of deep learning is Artificial Neural with perceptrons.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RefVsfAD5xAu",
        "colab_type": "text"
      },
      "source": [
        "### Perceptron\n",
        "\n",
        "Perceptron is the early artifical neural proposed in 1957 by Frank Rosenblatt. Its structure is smiliar with human's neuron. Nuerons activates and translate the signal when the input signal jumps over the activate potential. Perceptron use this concept using step function. So, the unit of artifical neuron using Logistic Regression is perceptron.\n",
        "\n",
        "Single-Layer Perceptron(SLP) is the basic perceptron that has multiple input layers and only one output layer.\n",
        "But SLP doesn't represnet XOR gate. To solve this problem, we use the next network.\n",
        "\n",
        "MultiLayer Perceptron(MLP) is the artificial neural network that has hidden layer. If MLP has more than 2 hidden layer, we call it Deep Neural Network, DNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RaJIMFH52Ml",
        "colab_type": "text"
      },
      "source": [
        "### Artifical Neural Network\n",
        "\n",
        "Artifical Neural Network, ANN is the component of artifical intelligence that is meant to simulate the functioning of a human brain.(Investopedia)\n",
        "\n",
        "Feed-Forward Neural Network(FFNN) is the basic ANN that all of connection betwwen node is forwarded, not back.\n",
        "If there are backward connections, we call it as Recurrent Neural Network(RNN).\n",
        "\n",
        "Fully-connected layer, FC means MLP of which any neuron were connected with all of the backward neuron. Dense Layer is the same meaning. If FFNN is fully connected, we call it as Fully-Connected FFNN.\n",
        "\n",
        "Activation Function is the function to represent the activate potential in human brain. Perceptron use the step function for activation function, but there are a lot of cases.\n",
        "Activation Function is not linear. If it is, the result between one hidden layer and more is not diffierent. So we use the different functions like the step function.\n",
        "\n",
        "Sigmoid function is the kind of activation function. but it was differentiated serveral times, its gradient is almost 0. This situation means front weight nodes make the effect less than back ones. It was called as Vanishing Gradient problem.\n",
        "\n",
        "Hyperbolic tangent function conver the input to the value between -1 and 1(Sigmoid one is between 0 and 1). It also has Vanishing Gradient problem when the value is closed with -1 or 1. But the other point's gradient is more than Sigmoid, so the probability of happening vanishing gradient is lower thant sigmoid.\n",
        "\n",
        "ReLU, Leaky ReLU is the function that has taken the gradient of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpzo3rWQ6CjW",
        "colab_type": "text"
      },
      "source": [
        "### How DL work\n",
        "\n",
        "#### Forward Propagation\n",
        "\n",
        "After the model is designed, input value go through input layer and hidden layer to output layer.\n",
        "\n",
        "#### Loss function\n",
        "\n",
        "Loss function calcuates the difference between the result of forward propagtion and the answer.\n",
        "\n",
        "The kinds of loss function are MSE, Cross-Entropy(we already learned).\n",
        "\n",
        "#### Optimizer\n",
        "\n",
        "Optimizer is the method to modify the value from the difference between the result and the answer.\n",
        "\n",
        "Batch Gradient Descent is the basic gradient descent. Each Epochs, it updates the all of arguments.\n",
        "\n",
        "Stochastic Gradient Descent only updates one data of arguments randomly.\n",
        "\n",
        "MiniBatch Gradient Descent updates a fixed amount data.\n",
        "\n",
        "Momentum is the method to find the global minimum using the inerita. Momentum used the certain value from the last descent value to calculate the now descent value.\n",
        "\n",
        "Adagrad is the method to apply the different learning rate for each arguments.\n",
        "\n",
        "RMSprop is the improved method from Adagrad which has a disadvantage that the learning rate is excessively low.\n",
        "\n",
        "Adam is the method that uses RMSprop and Momentum both.\n",
        "\n",
        "#### BackPropagation\n",
        "\n",
        "![Image](https://postfiles.pstatic.net/MjAyMDA4MjBfMTM1/MDAxNTk3OTI0OTkwNDMz.WpuUSsoQXcYxk5xohAxh_C_TfeWhqHgHPaLdy9yCVS8g.b_oXyVvRXsUNhO3_7eKDM6xjYZzPA0S-C-gvsp4v2fwg.PNG.conatus_recedendi/image.png?type=w966)\n",
        "\n",
        "$ h_{l,k} = AF(z_{l,k}) = AF(\\sum_{i=0}\n",
        "^{}W_{l-1,i,k}h_{l-1,i} + ... ) $\n",
        "\n",
        "\n",
        "$ w'_{l,k,n} = w_{l,k,n} - \\alpha\\frac{\\delta E_{total}}{\\delta w_{l,k,n}} = w_{l,k,n} - \\alpha(\\sum_{i=0}\\frac{\\delta E_{o_i}}{\\delta h_{l+1,n}})\\frac{\\delta h_{l+1,n}}{\\delta z_{l+1,n}}\\frac{\\delta z_{l+1,n}}{\\delta w_{l,k,n}} = w_{l,k,n} - \\alpha(\\sum_{i=0}\\frac{\\delta E_{o_i}}{\\delta h_{l+1,n}})\\frac{\\delta h_{l+1,n}}{\\delta z_{l+1,n}}h_{l,k}$\n",
        "\n",
        "$ o_i $  means $ h_{L,i} $ where L is the index of last layer. and $ AF(x) $ means activation function.\n",
        "\n",
        "In this formula, the first part can be represented with differentiated loss function and the second part with differentiated activation function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb-OmvWwLWSj",
        "colab_type": "text"
      },
      "source": [
        "### Overfitting\n",
        "\n",
        "Overfitting happens when the predicting model only suited for training data.\n",
        "\n",
        "To prevent overfitting, the following methods are used.\n",
        "\n",
        "1. Increasing the amount of data.\n",
        "\n",
        "2. Decreasing the complexity of the model.\n",
        "\n",
        "3. Applying the weight regularization.\n",
        "\n",
        "L1 regularization is adding $\\lambda |w| $ to the cost function. L1 regularization is used to find what properties affects on the model.\n",
        "\n",
        "L2 regularization is adding $\\frac{1}{2}\\lambda |w| $ to the cost function.\n",
        "\n",
        "4. Dropout\n",
        "\n",
        "Dropout is the method that in training steps some node(neuron) doesn't used not to make the model overfitted on the training set.\n",
        "Generally, Dropout is not used in prediction step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm_VK6E0O2HO",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Vanishing & Exploding\n",
        "\n",
        "Gradient Vanishing is the problem when the each gradient is low and then the nodes that are closed with the input node doesn't update well.\n",
        "\n",
        "Gradient Exploding is the reverse case of gradient vanishing.\n",
        "\n",
        "To prevent both cases, the following methods are used.\n",
        "\n",
        "1. ReLU Series\n",
        "\n",
        "2. Gradient Clipping\n",
        "\n",
        "3. Weight initialization\n",
        "\n",
        "The initial state of the model can be impact on the result. Xavier Initialization, He Initialization are used.\n",
        "\n",
        "4. Batch Normalization\n",
        "\n",
        "Batch Normalization is the method to make the inputs that entered each layers normalized with mean or distribution.\n",
        "\n",
        "5. Layer Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNhqUIJXRaB8",
        "colab_type": "text"
      },
      "source": [
        "### Example of ML Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joSRo8LuRFx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "outputId": "431fc51d-0414-4b2c-8d73-89543fb8bd74"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "newsdata = fetch_20newsgroups(subset='train')\n",
        "\n",
        "#print(newsdata.target_names)\n",
        "\n",
        "data = pd.DataFrame(newsdata.data, columns=['email'])\n",
        "data['target'] = pd.Series(newsdata.target)\n",
        "\n",
        "newsdata_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "train_email = data['email']\n",
        "train_label = data['target']\n",
        "test_email = newsdata_test.data\n",
        "test_label = newsdata_test.target\n",
        "\n",
        "max_words = 10000\n",
        "num_classes = 20\n",
        "\n",
        "def prepare_data(train_data, test_data, mode):\n",
        "    t = Tokenizer(num_words = max_words)\n",
        "    t.fit_on_texts(train_data)\n",
        "    X_train = t.texts_to_matrix(train_data, mode=mode)\n",
        "    X_test = t.texts_to_matrix(test_data, mode=mode)\n",
        "    return X_train, X_test, t.index_word\n",
        "\n",
        "X_train, X_test, index_to_word = prepare_data(train_email, test_email, 'binary')\n",
        "y_train = to_categorical(train_label, num_classes)\n",
        "y_test = to_categorical(test_label, num_classes)\n",
        "\n",
        "print('훈련 샘플 본문의 크기 : {}'.format(X_train.shape))\n",
        "print('훈련 샘플 레이블의 크기 : {}'.format(y_train.shape))\n",
        "print('테스트 샘플 본문의 크기 : {}'.format(X_test.shape))\n",
        "print('테스트 샘플 레이블의 크기 : {}'.format(y_test.shape))\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "def fit_and_evaluate(X_train, y_train, X_test, y_test):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_shape=(max_words,), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.1)\n",
        "    score = model.evaluate(X_test, y_test, batch_size=128, verbose=0)\n",
        "    return score[1]\n",
        "\n",
        "modes = ['binary', 'count', 'tfidf', 'freq']\n",
        "\n",
        "for mode in modes:\n",
        "    X_train, X_test, _ = prepare_data(train_email, test_email, mode)\n",
        "    score = fit_and_evaluate(X_train, y_train, X_test, y_test)\n",
        "    print(mode+\" Modes Test Accuarncy :\", score)\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 샘플 본문의 크기 : (11314, 10000)\n",
            "훈련 샘플 레이블의 크기 : (11314, 20)\n",
            "테스트 샘플 본문의 크기 : (7532, 10000)\n",
            "테스트 샘플 레이블의 크기 : (7532, 20)\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 2.2813 - accuracy: 0.3501 - val_loss: 0.9750 - val_accuracy: 0.8189\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 0.8566 - accuracy: 0.7673 - val_loss: 0.4524 - val_accuracy: 0.8878\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 3s 41ms/step - loss: 0.4262 - accuracy: 0.8855 - val_loss: 0.3528 - val_accuracy: 0.8966\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 3s 41ms/step - loss: 0.2585 - accuracy: 0.9314 - val_loss: 0.3230 - val_accuracy: 0.9072\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 4s 46ms/step - loss: 0.1715 - accuracy: 0.9576 - val_loss: 0.3222 - val_accuracy: 0.9072\n",
            "binary Modes Test Accuarncy : 0.825809895992279\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 4s 45ms/step - loss: 2.7477 - accuracy: 0.2454 - val_loss: 1.6355 - val_accuracy: 0.7482\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 4s 47ms/step - loss: 1.4412 - accuracy: 0.6305 - val_loss: 0.7012 - val_accuracy: 0.8542\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 3s 42ms/step - loss: 0.7670 - accuracy: 0.8019 - val_loss: 0.4877 - val_accuracy: 0.8843\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 3s 40ms/step - loss: 0.5140 - accuracy: 0.8774 - val_loss: 0.4194 - val_accuracy: 0.8922\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 3s 41ms/step - loss: 0.4594 - accuracy: 0.9043 - val_loss: 0.4172 - val_accuracy: 0.8922\n",
            "count Modes Test Accuarncy : 0.8122676610946655\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 4s 50ms/step - loss: 2.3171 - accuracy: 0.3422 - val_loss: 0.8511 - val_accuracy: 0.8445\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 3s 40ms/step - loss: 0.8810 - accuracy: 0.7572 - val_loss: 0.4325 - val_accuracy: 0.8975\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 3s 41ms/step - loss: 0.4437 - accuracy: 0.8800 - val_loss: 0.3607 - val_accuracy: 0.9072\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 3s 42ms/step - loss: 0.3070 - accuracy: 0.9250 - val_loss: 0.3186 - val_accuracy: 0.9214\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 4s 46ms/step - loss: 0.2126 - accuracy: 0.9470 - val_loss: 0.3490 - val_accuracy: 0.9205\n",
            "tfidf Modes Test Accuarncy : 0.8339086771011353\n",
            "Epoch 1/5\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 2.9784 - accuracy: 0.0848 - val_loss: 2.9257 - val_accuracy: 0.1572\n",
            "Epoch 2/5\n",
            "80/80 [==============================] - 4s 44ms/step - loss: 2.7222 - accuracy: 0.2025 - val_loss: 2.4152 - val_accuracy: 0.3993\n",
            "Epoch 3/5\n",
            "80/80 [==============================] - 3s 44ms/step - loss: 2.2239 - accuracy: 0.3045 - val_loss: 1.9315 - val_accuracy: 0.5283\n",
            "Epoch 4/5\n",
            "80/80 [==============================] - 3s 42ms/step - loss: 1.7905 - accuracy: 0.4386 - val_loss: 1.5245 - val_accuracy: 0.6696\n",
            "Epoch 5/5\n",
            "80/80 [==============================] - 3s 43ms/step - loss: 1.4353 - accuracy: 0.5654 - val_loss: 1.2113 - val_accuracy: 0.7429\n",
            "freq Modes Test Accuarncy : 0.6891927719116211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftcuc2PTSX-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modeling\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabular, output_dim, input_length))\n",
        "                                      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYZA59n557JD",
        "colab_type": "text"
      },
      "source": [
        "## Reference\n",
        "- [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/22882)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGqd2zzioq4P",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}